---
title: "Model"
author: "Ivan Specht"
date: "2/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

SECTION ONE: Preliminaries

Sub-section 1. Load and clean data
```{r}
# Relevant libraries
library(igraph)
library(lubridate)
library(Rfast)
library(mixdist)
library(parallel)
library(ggplot2)
library(ggraph)
library(scales)
library(gridExtra)

# Set directory to the location of "OO-modeling" folder
setwd("~/Desktop/OO-modeling")

# Read in OO simulation backend
data <- read.csv("./histories_CMU.csv")

# Read in key to convert between simulation IDs and P2P IDs
leger <- read.csv("./participants_CMU.csv")
leger[,"p2p_id"] <- paste(leger[,"p2p_id"])
data[,"peer_id"] <- paste(data[,"peer_id"])

# Limit data to contacts only
contacts <- subset(data, type == "contact")

# Reformat contact duration
contacts$contact_length <- round(milliseconds(contacts$contact_length))

# Compute end time per contact
end_time <- contacts$time + contacts$contact_length

# Subset dataset to relevant categories
contacts <- contacts[, c("user_id", "peer_id", "time")]
contacts <- cbind(contacts, end_time)

# Convert P2P ID to sim ID
convert_peer_ID <- function(x, leger){
  return(leger[which(leger[,"p2p_id"] == x), "id"][1])
}

# Convert old ID to new sim ID
convert_user_ID <- function(x, leger){
  return(
    leger$id[which(leger$p2p_id == leger$p2p_id[which(leger$id == x)])[1]]
  )
}

contacts[,"user_id"] <- sapply(contacts[,"user_id"], convert_user_ID, leger = leger)
contacts[,"peer_id"] <- sapply(contacts[,"peer_id"], convert_peer_ID, leger = leger)

# Reformat contact time
contacts$time <- as.POSIXct(contacts$time, origin = "1970-01-01", tz = "America/Denver")
contacts$end_time <- as.POSIXct(contacts$end_time, origin = "1970-01-01", tz = "America/Denver")

# Make user ID consistently less than peer ID
lower <- pmin(contacts$user_id, contacts$peer_id)
upper <- pmax(contacts$user_id, contacts$peer_id)
contacts$user_id <- lower
contacts$peer_id <- upper

contacts <- contacts[with(contacts, order(user_id, peer_id, time)), ]

# Length between contacts to merge into 1 contact
gap <- 30 #seconds

# Merge same contacts
i=2
while (i < nrow(contacts)) {
  if(
    contacts[i,"user_id"] == contacts[i-1,"user_id"] & 
    contacts[i,"peer_id"] == contacts[i-1,"peer_id"] & 
    contacts[i,"time"] < (contacts[i-1,"end_time"] + gap)
  ){
    contacts[i-1, "end_time"] <- max(contacts[i-1, "end_time"], contacts[i, "end_time"])
    contacts <- contacts[-i, ]
  }else{
    i <- i+1
  }
}

# Clean up datasheet and merge in relevant metrics
contacts <- contacts[with(contacts, order(time)), ]
row.names(contacts) <- 1:nrow(contacts)
duration <- contacts$end_time - contacts$time
contacts <- cbind(contacts, duration)
```

Sub-section 2. Process "contacts" into time series data useful for epi, over the course of one week
```{r}
# Eliminate contacts shorter than this duration
min_duration <- 60 #seconds

# Create new dataset for the contacts we're keeping (ts = time series)
ts <- subset(contacts, duration >= min_duration)

# Who escaped?
escape <- subset(data, out == "ESCAPED")
escape <- escape[, c("user_id", "time")]
escape$time <- as.POSIXct(escape$time, origin = "1970-01-01", tz = "America/Denver")
escape <- escape[order(escape$time),]
escape[,"user_id"] <- sapply(escape[,"user_id"], convert_user_ID, leger = leger)
row.names(escape) <- 1:nrow(escape)

# When did the simulation start?
start = as.POSIXct(min(contacts$time), origin = "1970-01-01", tz = "America/Denver")

# When did the simulation end?
end <- as.POSIXct(max(contacts$time), origin = "1970-01-01", tz = "America/Denver")

# Who escaped BEFORE the end?
escapees <- escape$user_id[escape$time < end]

# All participants
leger[,"id"] <- sapply(leger[,"id"], convert_user_ID, leger = leger)
users <- setdiff(leger$id, escapees)

# Subset ts
ts <- subset(ts, (user_id %in% users & peer_id %in% users))
ts <- subset(ts, time < end)

# Convert to 1-week format
ts$time <- difftime(ts$time, start, units = "secs")
ts$end_time <- difftime(ts$end_time, start, units = "secs")

for (i in which(ts$end_time > 604800)) {
  old <- ts[i,]
  ts$end_time[i] <- 604800
  ts$duration[i] <- 604800 - ts$time[i]
  old$time = 0
  old$end_time <- old$end_time - 604800
  old$duration <- old$end_time
  ts <- rbind(ts, old)
}

ts <- ts[order(ts$time),]
row.names(ts) <- 1:nrow(ts)

ts$time <- seconds(ts$time)
ts$end_time <- seconds(ts$end_time)
```

SECTION TWO.

Sub-section 1. Demonstrate proportionate mixing assumption, as described in paper
```{r}

# Adjacency matrix of total duration of contact(s)
real_adj <- matrix(0, nrow = length(users), ncol = length(users))

for (i in 1:length(users)) {
  for (j in i:length(users)) {
    sub <- subset(
      ts,
      (user_id == users[i] & peer_id == users[j]) | (user_id == users[j] & peer_id == users[i])
    )
    real_adj[i,j] <- sum(sub$duration)
    real_adj[j,i] <- sum(sub$duration)
  }
}

# Binary adjacency matrix
bin_adj <- real_adj > 0

# Node degrees
deg <- colsums(bin_adj)

# Predicted adjacency matrix under proportionate mixing, up to a constant
pred_adj <- outer(deg,deg)/sum(deg)
diag(pred_adj) <- 0

# Convert to numeric form...
preds <- as.numeric(pred_adj)
deps <- as.numeric(bin_adj)

# Linear model: decent R^2, low p-value
lmod <- lm(deps~preds+0)
summary(lmod)

# High correlation:
cor(preds, deps)

```

Sub-section 2: show low correlation between degree and length of contact

```{r}
# predictor variables: proportionate mixing, filtered for nonzero contact length
preds2 <- preds[deps>0]

# predictee variables: total duration of contact, filtered for nonzero contact length
deps2 <- as.numeric(real_adj)[deps>0]

# Linear model: low R^2
lmod2 <- lm(deps2~preds2+0)
summary(lmod2)

# Low correlation
cor(preds2, deps2)
```

Sub-section 3: compare simulated clustering coefficients and path length versus actual
```{r}

graph_stats <- function(xxxx){
  
  toy_adj <- pred_adj
  binaries <- runif(nrow(toy_adj) * (nrow(toy_adj) - 1) / 2) < upper_tri(toy_adj)
  toy_adj[upper.tri(toy_adj)] <- binaries
  toy_adj <- t(toy_adj)
  toy_adj[upper.tri(toy_adj)] <- binaries
  
  toy_g <- graph_from_adjacency_matrix(toy_adj[colsums(toy_adj) > 0, colsums(toy_adj) > 0], mode = "undirected")
  
  return(
      c(
      transitivity(toy_g),
      average.path.length(toy_g)
    )
  )

}

res <- matrix(unlist(mclapply(1:10000, graph_stats, mc.cores = 12)), ncol = 2, byrow = TRUE)

mean(res[,1])
quantile(res[,1], c(0.025, 0.975), type = 1)

mean(res[,2])
quantile(res[,2], c(0.025, 0.975), type = 1)

real_g <- graph_from_adjacency_matrix(bin_adj, mode = "undirected")

transitivity(real_g)
average.path.length(real_g)
length(E(real_g))

```


SECTION FOUR: Visualization

Figure 1: number of contacts, 1st and 2nd degree
```{r}
deg2 <- c()
for(i in 1:length(users)){
  deg2[i] <- sum(rowsums(bin_adj[-i, which(bin_adj[i,] == 1), drop = FALSE]) > 0)
}

dat <- data.frame(Contacts = c(deg, deg2), Degree = c(rep("First-degree", length(users)), rep("Second-degree", length(users))))

ggplot(dat, aes(x=Contacts, fill=Degree)) +
  geom_histogram(alpha=0.5, breaks = seq(0,152,4), position = "identity") + 
  labs(
    title = "First- and Second-Degree Contacts, CMU",
    x = "Contacts",
    y = "Frequency"
  ) + scale_color_brewer(palette="Dark2") + scale_fill_brewer(palette="Dark2") +
theme_minimal()

ggsave("./out/figure1a.pdf", width = 6.5, height = 4)

mean(deg2)
sd(deg2)
range(deg2)

```

Figure 2: when people are interacting
```{r}
hour_interactions <- c()
hour_breaks <- seconds(seq(0, as.numeric(end - start)*24*60*60, 3600))
for (i in 2:length(hour_breaks)) {
  sub <- subset(ts, hour_breaks[i-1] <= time & time < hour_breaks[i])
  sub <- unique(sub[,1:2])
  hour_interactions[i-1] <- nrow(sub)
}

timebins <- start + hour_breaks[1:(length(hour_breaks) - 1)]

date_labels <- seq(as.Date("2020-10-29"), as.Date("2020-11-03"), "day")
date_labels <- mapply(paste, wday(date_labels, label = T), month(date_labels, label = T), day(date_labels), rep("12:00am", length(date_labels)))

ggplot(data.frame(t = timebins, count = hour_interactions), aes(x=t, y = count)) + 
  geom_line() + 
  labs(
    title = "Timing of Interactions, CMU",
    x = "Time",
    y = "Number of Interactions"
  ) + 
  scale_x_datetime(breaks = date_breaks(width = "1 day"), date_labels = date_labels) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = -45, vjust = 1, hjust=0), plot.margin=unit(c(5.5,42,5.5,5.5), "pt")) 

ggsave("./out/figure2a.pdf", width = 6.5, height = 4)


```

Figure 3: Scatterplot of 1st and 2nd degree contacts
```{r}

get_p <- function(params){
  mean((deg2 - params[1] * (1 - exp(-deg * params[2])))^2)
}

best_p <- optim(c(238,0.085), get_p)$par

RMSE <- sqrt(get_p(best_p))

mod_fn <- function(x){
  best_p[1] * (1 - exp(-x * best_p[2]))
}

dat2 <- data.frame(x = deg, y = deg2)

### which estimates are best, most above, most below?
# modeled values
mod_vals <- sapply(deg, mod_fn)

# pull out these special points and recolor them
scatter_colors <- rep("black", nrow(dat2))

ggplot(dat2, aes(x=x,y=y)) +
  geom_function(fun = mod_fn, color = "red") + 
  geom_point(color = scatter_colors) + 
  theme_minimal() +
  labs(
    title = "First- and Second-Degree Contacts, CMU",
    x = "First-Degree Contacts",
    y = "Second-Degree Contacts"
  )

ggsave("./out/figure3a.pdf", width = 6.5, height = 4)

print(mean(deg))
print(sd(deg))
print(range(deg))

print(best_p)
print(RMSE)


```

Figure 5: regression analyses
```{r}

# Pull in results from Small_Model_BYU.R
load("./final_CMU.RData")

# Time-weighted second degree contact matrix
mat2 <- real_adj %*% real_adj
diag(mat2) <- 0

# Regression results table
table_reg <- data.frame()

# equal weight
mod <- summary(lm(final ~ deg + 0))
table_reg <- rbind(
  table_reg,
  c(
    "Equal-Weighted Contacts",
    paste(round(mod$adj.r.squared, 3)),
    paste(round(mod$coefficients[3], 3)),
    "***" # copied based on results
  )
)

# duration weight
mod <- summary(lm(final ~ colsums(real_adj) + 0))
table_reg <- rbind(
  table_reg,
  c(
    "Duration-Weighted Contacts",
    paste(round(mod$adj.r.squared, 3)),
    paste(round(mod$coefficients[3], 3)),
    "***" # copied based on results
  )
)

# duration weight, 1st + 2nd degree
mod <- summary(lm(final ~ colsums(real_adj) + colsums(mat2) + 0))
table_reg <- rbind(
  table_reg,
  c(
    "Duration-Weighted 1st- and 2nd-Degree Contacts",
    paste(round(mod$adj.r.squared, 3)),
    paste(round(mod$coefficients[,3], 3), collapse = ", "),
    "***, ***" # copied based on results
  )
)

colnames(table_reg) <- c("Predictor(s)", "Adjusted R-Squared", "t Statistic(s)", "Significance Code(s)")

tg = gridExtra::tableGrob(table_reg, rows = NULL)
h = grid::convertHeight(sum(tg$heights), "in", TRUE)
w = grid::convertWidth(sum(tg$widths), "in", TRUE)

pdf("./out/figure5a.pdf", width=w, height=h)
grid.table(table_reg, rows = NULL)
dev.off()


```


