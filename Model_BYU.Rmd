---
title: "Model"
author: "Ivan Specht"
date: "2/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

SECTION ONE: Preliminaries

Sub-section 1. Load and clean data
```{r}
# Relevant libraries
library(igraph)
library(lubridate)
library(Rfast)
library(mixdist)
library(parallel)
library(ggplot2)
library(ggraph)
library(scales)
library(gridExtra)

# Set directory to the location of "OO-modeling" folder
setwd("~/Desktop/OO-modeling")

# Read in OO simulation backend
data <- read.csv("./histories_BYU.csv")

# Read in key to convert between simulation IDs and P2P IDs
leger <- read.csv("./participants_BYU.csv")
leger[,"p2p_id"] <- paste(leger[,"p2p_id"])
data[,"peer_id"] <- paste(data[,"peer_id"])

# Limit data to contacts only
contacts <- subset(data, type == "contact")

# Reformat contact duration
contacts$contact_length <- round(milliseconds(contacts$contact_length))

# Compute end time per contact
end_time <- contacts$time + contacts$contact_length

# Subset dataset to relevant categories
contacts <- contacts[, c("user_id", "peer_id", "time")]
contacts <- cbind(contacts, end_time)

# Convert P2P ID to sim ID
convert_peer_ID <- function(x, leger){
  return(leger[which(leger[,"p2p_id"] == x), "id"][1])
}
contacts[,"peer_id"] <- sapply(contacts[,"peer_id"], convert_peer_ID, leger = leger)

# Reformat contact time
contacts$time <- as.POSIXct(contacts$time, origin = "1970-01-01", tz = "America/Denver")
contacts$end_time <- as.POSIXct(contacts$end_time, origin = "1970-01-01", tz = "America/Denver")

# Make user ID consistently less than peer ID
lower <- pmin(contacts$user_id, contacts$peer_id)
upper <- pmax(contacts$user_id, contacts$peer_id)
contacts$user_id <- lower
contacts$peer_id <- upper

contacts <- contacts[with(contacts, order(user_id, peer_id, time)), ]

# Length between contacts to merge into 1 contact
gap <- 30 #seconds

# Merge same contacts
i=2
while (i < nrow(contacts)) {
  if(
    contacts[i,"user_id"] == contacts[i-1,"user_id"] & 
    contacts[i,"peer_id"] == contacts[i-1,"peer_id"] & 
    contacts[i,"time"] < (contacts[i-1,"end_time"] + gap)
  ){
    contacts[i-1, "end_time"] <- max(contacts[i-1, "end_time"], contacts[i, "end_time"])
    contacts <- contacts[-i, ]
  }else{
    i <- i+1
  }
}

# Clean up datasheet and merge in relevant metrics
contacts <- contacts[with(contacts, order(time)), ]
row.names(contacts) <- 1:nrow(contacts)
duration <- contacts$end_time - contacts$time
contacts <- cbind(contacts, duration)
```

Sub-section 2. Process "contacts" into time series data useful for epi, over the course of one week
```{r}
# Eliminate contacts shorter than this duration
min_duration <- 60 #seconds

# Create new dataset for the contacts we're keeping (ts = time series)
ts <- subset(contacts, duration >= min_duration)

# Who escaped?
escape <- subset(data, out == "ESCAPED")
escape <- escape[, c("user_id", "time")]
escape$time <- as.POSIXct(escape$time, origin = "1970-01-01", tz = "America/Denver")
escape <- escape[order(escape$time),]
row.names(escape) <- 1:nrow(escape)

# When did the simulation start?
start = as.POSIXct(min(contacts$time), origin = "1970-01-01", tz = "America/Denver")

# Limit to 1 week of contact data
end <- start + weeks(1)

# Who escaped BEFORE one week elapsed
escapees <- escape$user_id[escape$time < end]

# All participants
users <- setdiff(leger$id, escapees)

# Subset ts
ts <- subset(ts, (user_id %in% users & peer_id %in% users))
ts <- subset(ts, time < end)

# Convert to 1-week format
ts$time <- difftime(ts$time, start, units = "secs")
ts$end_time <- difftime(ts$end_time, start, units = "secs")

for (i in which(ts$end_time > 604800)) {
  old <- ts[i,]
  ts$end_time[i] <- 604800
  ts$duration[i] <- 604800 - ts$time[i]
  old$time = 0
  old$end_time <- old$end_time - 604800
  old$duration <- old$end_time
  ts <- rbind(ts, old)
}

ts <- ts[order(ts$time),]
row.names(ts) <- 1:nrow(ts)

ts$time <- seconds(ts$time)
ts$end_time <- seconds(ts$end_time)
```

SECTION TWO.

Sub-section 1. Demonstrate proportionate mixing assumption, as described in paper
```{r}

# Adjacency matrix of total duration of contact(s)
real_adj <- matrix(0, nrow = length(users), ncol = length(users))

for (i in 1:length(users)) {
  for (j in i:length(users)) {
    sub <- subset(
      ts,
      (user_id == users[i] & peer_id == users[j]) | (user_id == users[j] & peer_id == users[i])
    )
    real_adj[i,j] <- sum(sub$duration)
    real_adj[j,i] <- sum(sub$duration)
  }
}

# Binary adjacency matrix
bin_adj <- real_adj > 0

# Node degrees
deg <- colsums(bin_adj)

# Predicted adjacency matrix under proportionate mixing, up to a constant
pred_adj <- outer(deg,deg)/sum(deg)
diag(pred_adj) <- 0

# Convert to numeric form...
preds <- as.numeric(pred_adj)
deps <- as.numeric(bin_adj)

# Linear model: decent R^2, low p-value
lmod <- lm(deps~preds+0)
summary(lmod)

# High correlation:
cor(preds, deps)

```

Sub-section 2: show low correlation between degree and length of contact

```{r}
# predictor variables: proportionate mixing, filtered for nonzero contact length
preds2 <- preds[deps>0]

# predictee variables: total duration of contact, filtered for nonzero contact length
deps2 <- as.numeric(real_adj)[deps>0]

# Linear model: low R^2
lmod2 <- lm(deps2~preds2+0)
summary(lmod2)

# Low correlation
cor(preds2, deps2)
```

Sub-section 3: compare simulated clustering coefficients and path length versus actual
```{r}

graph_stats <- function(xxxx){
  
  toy_adj <- pred_adj
  binaries <- runif(nrow(toy_adj) * (nrow(toy_adj) - 1) / 2) < upper_tri(toy_adj)
  toy_adj[upper.tri(toy_adj)] <- binaries
  toy_adj <- t(toy_adj)
  toy_adj[upper.tri(toy_adj)] <- binaries
  
  toy_g <- graph_from_adjacency_matrix(toy_adj[colsums(toy_adj) > 0, colsums(toy_adj) > 0], mode = "undirected")
  
  return(
      c(
      transitivity(toy_g),
      average.path.length(toy_g)
    )
  )

}

res <- matrix(unlist(mclapply(1:10000, graph_stats, mc.cores = 12)), ncol = 2, byrow = TRUE)

mean(res[,1])
quantile(res[,1], c(0.025, 0.975), type = 1)

mean(res[,2])
quantile(res[,2], c(0.025, 0.975), type = 1)

```




SECTION THREE. Stochastic bootstrap model.

Sub-section 1: create bootstrap sampling function for contact times
```{r}
# Length of simulation, in weeks
sim_duration <- 4 # WEEKS!!!!

# Length of simulation, in seconds
sim_duration_secs <- sim_duration*60*60*24*7

# Time of simulation start, seconds
start_secs <- second(start) + 60*minute(start) + 60*60*hour(start)

# List of all contacts
edgelist <- unique(ts[,1:2])
row.names(edgelist) <- 1:nrow(edgelist)

# Start times of contacts
c_start <- list()

# End times of contacts
c_end <- list()

total_time <- c()

for (i in 1:nrow(edgelist)) {
  # Get all interactions for a given pair
  sub <- subset(ts, user_id == edgelist[i,1] & peer_id == edgelist[i,2])
  
  # Extract the start times of the interactions
  c_start[[i]] <- as.numeric(sub$time + rep(seconds(weeks(0:(sim_duration))), each = nrow(sub))) # adding extra week for time offset
  
  # Extract the end times of the interactions
  c_end[[i]] <- as.numeric(sub$end_time + rep(seconds(weeks(0:(sim_duration))), each = nrow(sub))) # adding extra week for time offset
  
  # Total time in contact for the pair
  total_time[i] <- sum(c_end[[i]] - c_start[[i]])/((sim_duration+1)*60*60*24*7)
}

# v0 is infection time of infector
# i is index of edge contact times

# Compute parameters of the Weibull distribution of the generation interval
wpar1 <- as.numeric(weibullpar(5, 1.9)[1])
wpar2 <- as.numeric(weibullpar(5, 1.9)[2])


# This function stochastically generates the TIME of transmission between an infectious agent and a susceptible one, given the start time(s) and end time(s) if their interactions.

# pstarts is the CDF of the generation interval distribution evaluated at the start time(s) of the contact(s), relative to the time the infectious agent contracts the virus
# pends is the CDF of the generation interval distribution evaluated at the end time(s) of the contact(s), relative to the time the infectious agent contracts the virus
# int is total duration of contact

# We use the probability integral transform to do the calculation
get_time <- function(pstarts, pends, int){
  
  if(length(pstarts) > 1){
    breaks <- pstarts - c(0, pends[1:(length(pends) - 1)])
  }else{
    breaks <- pstarts
  }
  
  divs <- pstarts - cumsum(breaks)
  
  pick <- runif(1, 0, int)
  adj_pick <- pick + sum(breaks[which(divs < pick)])
  
  return(
    qweibull(adj_pick, wpar1, wpar2) * 86400
  )
}


# This function stochastically decides WHETHER a transmission occurs, and if so, returns the time of transmission; otherwise returns Inf

# i is the index used to bootstrap the interaction times between two agents
# neighbor is the possible recipient of infection
# v0 is the time of infection for the index case
# i0 is the index case
# vaxed stores which agents are vaccinated
# masked stores which agents are masked
# daily tests is the number of tests per day
# test rate id the frequency with which EACH AGENT gets tested, which may be based on OO data
# turnaround is test turnaround time
# start_curfew is the time (daily) at which we start curfew, i.e. no interactions may occur (unused)
# end_curfew is the time (daily) at which we end curfew (unused)
# p_curfew is the probability that curfew is obeyed (unused)

transmission_time <- function(i, neighbor, v0, i0, vaxed, masked, daily_tests, test_rate, turnaround, start_curfew, end_curfew, p_curfew){
  
  # convert to days
  starts <- (c_start[[i]] - v0)/86400
  ends <- (c_end[[i]] - v0)/86400
  
  # take into account quarantine due to test
  if(daily_tests > 0){
    start_quarantine <- rexp(1, test_rate[i0]) + turnaround
    starts <- starts[starts < start_quarantine]
    if(length(starts) > 0){
      ends <- ends[1:length(starts)]
      ends <- pmin(ends, start_quarantine)
    }else{
      ends <- numeric(0)
    }
  }
  
  if(runif(1) < p_curfew){
    start_times <- (starts + (v0 + start_secs)/86400) %% 1 # time of day, fraction of day
  
    starts <- starts[(start_times < start_curfew) | (start_times >= end_curfew)]
    ends <- ends[(start_times < start_curfew) | (start_times >= end_curfew)]
  }
  
  pstarts <- pweibull(starts, wpar1, wpar2)
  pends <- pweibull(ends, wpar1, wpar2)
  
  int <- sum(pends - pstarts)
  
  prob <- 1 - exp(-lambda*int)
  
  # UNUSED-----
  
  if(masked[neighbor]){
    prob <- prob*0.3
  }
  
  if(masked[i0]){
    prob <- prob*0.3
  }
  
  # End unused------
  
  if(vaxed[neighbor]){
    prob <- prob * 0.05
  }
  
  if(runif(1) < prob){
    return(
      round(
        get_time(pstarts = pstarts, pends = pends, int = int)
      ) + v0
    )
  }else{
    return(Inf)
  }
}

# Stochastic function that spreads virus to neighbors (or doesn't)

# neighbors is all the neighbors of i0
spread <- function(v0, neighbors, i0, vaxed, masked, daily_tests, test_rate, turnaround, start_curfew, end_curfew, p_curfew){
  
  # uniformly sample a time series for each neighbor, i.e. bootstrap
  indices <- sample(1:nrow(edgelist), size = length(neighbors), replace = TRUE)
  
  mapply(transmission_time, indices, neighbors, MoreArgs = list(v0 = v0, i0 = i0, vaxed = vaxed, masked = masked, daily_tests = daily_tests, test_rate = test_rate, turnaround = turnaround, start_curfew = start_curfew, end_curfew = end_curfew, p_curfew = p_curfew))
}

# Get neighbors stochastically.
get_neighbors <- function(i0, activity, sum_activity){
  # Choose neighbors probabilistically based on OO activity score
  which(runif(N) < ((activity[i0] * activity / sum_activity)))
}

# Helper function for getting test rate
# Test rate is computed as 1 - exp(- constant * activity)
get_const <- function(const, activity){
  (sum(1 - exp(-const * activity)) - daily_tests)^2
}


```

Sub-section 2: Outbreak setup
```{r}
# Number of agents
N = 6000

# First, we fit a NBin distribution to the number of contacts based on the OO data.
lik <- function(params, deg){
  -sum(log(dnbinom(deg, params[1], params[2])))
}
est <- suppressWarnings(
  nlm(lik, p = c(mean(deg)^2/(var(deg) - mean(deg)), mean(deg)/var(deg)), deg=deg)
)$estimate

# Probability of someone's neighbor being included in sample
p <- 1/3

# Now we assume the number of contacts an agent makes within the WHOLE population also follows a NBin(r,p) distribution
# p parameter for full number of contacts
p_contact_dist <- p*est[2]/(1-est[2]*(1-p))

# r parameter for full number of contacts
r_contact_dist <- est[1]

# Mean and variance in number of contacts - full populaution
mu = (1 - p_contact_dist)*r_contact_dist/p_contact_dist
sigma2 = (1 - p_contact_dist)*r_contact_dist/(p_contact_dist^2)


# Calibrate lambda parameter

R0 <- 2 # basic reproductive number

# loss function
err <- function(lambda){
  (mean(1 - exp(-lambda * total_time)) - R0/((sigma2 + mu^2)/mu - 1))^2
}

lambda <- nlm(err, R0/(((sigma2 + mu^2)/mu - 1) * mean(total_time)))$estimate


# Compute activity levels

# We assume these activity levels follow a Gamma distribution.
# The reason for this assumption is that if the number of outward transmissions for agent i is approximately Poisson(activity[i]), then the unconditional distribution in the number of onwards transmissions across all i is NBin, as desired. Therefore, the Gamma distribution is the correct choice for the distribution of activity levels.

# Here we compute the shape and rate parameters for the activity level distribution
shape <- mu^2/(sigma2 - mu)
rate <- mu/(sigma2 - mu)

# Default model parameters
default <- list(
  p_vax = 0,
  p_mask = 0,
  vax_strategic = FALSE,
  daily_tests = 0,
  test_strategic = FALSE,
  test_sens = 0.87,
  turnaround = 1,
  p_curfew = 0,
  curfew_strategic = FALSE
)

# when does strategic curfew start, daily? seconds past start time of sim
start_curfew_strategic <- as.numeric(difftime(force_tz(date(start) + hours(10), "America/Denver"), start, units = "secs"))

# when does strategic curfew end, daily? seconds past start time of sim
end_curfew_strategic <- as.numeric(difftime(force_tz(date(start) + hours(13), "America/Denver"), start, units = "secs"))

```

Sub-section 3. Outbreak function
```{r}
# Stochastically generate an outbreak

outbreak <- function(
  xxxx, # useless argument, needed to run mclapply
  p_vax = default$p_vax, 
  p_mask = default$p_mask,
  vax_strategic = default$vax_strategic, 
  daily_tests = default$daily_tests, 
  test_strategic = default$test_strategic, 
  test_sens = default$test_sens,
  turnaround = default$turnaround, 
  p_curfew = default$p_curfew, 
  curfew_strategic = default$curfew_strategic
) {

  # Generate activity levels
  activity <- rgamma(N, shape = shape, rate = rate)
  sum_activity <- sum(activity)
  
  # who is vaccinated?
  if(vax_strategic){
    vaxed <- 1:N %in% sample(N, round(N*p_vax), prob = activity)
  }else{
    vaxed <- 1:N %in% sample(N, round(N*p_vax))
  }
  
  # Frequency of testing
  if(daily_tests > 0){
    if(test_strategic){
      const <- nlm(get_const, p = daily_tests/sum_activity, activity = activity)$estimate
      test_rate <- 1 - exp(-const * activity)
    }else{
      test_rate <- rep(daily_tests/N, N)
    }
  }else{
    test_rate <- rep(0, N)
  }
  test_rate <- test_rate * test_sens
  
  # Who wears a mask?
  masked <- rbinom(N, 1, p_mask)
  
  # When is curfew? (unused)
  if(curfew_strategic){
    # when does strategic curfew start, daily? fraction of day
    start_curfew <- 10/24
    
    # when does strategic curfew end, daily? hours into day
    end_curfew <- start_curfew + (3/24)
  }else{
    start_curfew <- runif(1)
    
    end_curfew <- (start_curfew + 3/24) %% 1
  }
  
  #######

  # infection status for each agent (binary)
  inft <- rep(0, N)
  
  # infection time for each agent (seconds)
  inft_time <- rep(Inf, N)
  
  # who did we already spread the virus to?
  checklist <- rep(0, N)
  
  # when in the cycle of a week do we start?
  adjust <- floor(runif(1, 0, 604800))

  # one index case
  seed <- sample(N, 1, prob = activity)
  inft[seed] <- 1
  inft_time[seed] <- adjust
  
  first = TRUE
  
  while (sum(inft) > sum(checklist)) {
    # For whom do we need to generate infections?
    who <- which((inft - checklist) == 1)
    
    all_v0 <- inft_time[who]
    neighbors <- lapply(who, get_neighbors, activity = activity, sum_activity = sum_activity)
    
    # When do we transmit, if at all?
    transmit_times <- mapply(spread, all_v0, neighbors, who, MoreArgs = list(vaxed = vaxed, masked = masked, daily_tests = daily_tests, test_rate = test_rate, turnaround = turnaround, start_curfew = start_curfew, end_curfew = end_curfew, p_curfew = p_curfew), SIMPLIFY = FALSE)
    
    # Update who's infected and update checklist
    for (i0 in 1:length(who)) {
      inft_time[neighbors[[i0]]] <- pmin(inft_time[neighbors[[i0]]], transmit_times[[i0]])
      inft[which(inft_time < (sim_duration_secs + adjust))] <- 1
    }
    checklist[who] <- 1
    
    if(first == TRUE){
      repro <- sum(inft) - 1
      first <- FALSE
    }
  }
  
  # get total infections after 4 weeks
  return(c(repro, sum(inft)))

}

```

Sub-section 4. MC simulation.

```{r}
# MC simulation at 10000 reps

results <- matrix(unlist(mclapply(1:10000, outbreak, mc.cores = 12)), ncol = 2, byrow = TRUE)
#save(results, file = "results_BYU.RData")

```

Sub-section 5. summary stats
```{r}

R_eff <- mean(results[,1])
R_sd <- sd(results[,1])

mean_cumulative <- mean(results[,2])
median_cumulative <- median(results[,2])
cri_cumulative <- quantile(results[,2], c(0.025, 0.975), type = 1)

R_eff
R_sd

mean_cumulative
median_cumulative
cri_cumulative


est <- suppressWarnings(
  nlm(lik, p = c(mean(results[,1])^2/(var(results[,1]) - mean(results[,1])), mean(results[,1])/var(results[,1])), deg=results[,1])
)$estimate

```

SECTION FOUR: Visualization

Figure 1: number of contacts, 1st and 2nd degree
```{r}
deg2 <- c()
for(i in 1:length(users)){
  deg2[i] <- sum(rowsums(bin_adj[-i, which(bin_adj[i,] == 1), drop = FALSE]) > 0)
}

dat <- data.frame(Contacts = c(deg, deg2), Degree = c(rep("First-degree", length(users)), rep("Second-degree", length(users))))

ggplot(dat, aes(x=Contacts, fill=Degree)) +
  geom_histogram(alpha=0.5, breaks = seq(0,264,4), position = "identity") + 
  labs(
    title = "First- and Second-Degree Contacts, BYU",
    x = "Contacts",
    y = "Frequency"
  ) + scale_color_brewer(palette="Dark2") + scale_fill_brewer(palette="Dark2") +
theme_minimal()

ggsave("./out/figure1b.pdf", width = 6.5, height = 4)

mean(deg2)
sd(deg2)
range(deg2)

```

Figure 2: when people are interacting
```{r}
hour_interactions <- c()
hour_breaks <- seconds(seq(0, 604800, 3600))
for (i in 2:length(hour_breaks)) {
  sub <- subset(ts, hour_breaks[i-1] <= time & time < hour_breaks[i])
  sub <- unique(sub[,1:2])
  hour_interactions[i-1] <- nrow(sub)
}

timebins <- start + hour_breaks[1:(length(hour_breaks) - 1)]

ggplot(data.frame(t = timebins, count = hour_interactions), aes(x=t, y = count)) + 
  geom_line() + 
  labs(
    title = "Timing of Interactions, BYU",
    x = "Time",
    y = "Number of Interactions"
  ) + 
  scale_x_datetime(breaks = date_breaks(width = "1 day"), date_labels = "%a %h %d 12:00am") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = -45, vjust = 1, hjust=0), plot.margin=unit(c(5.5,22,5.5,5.5), "pt")) 

ggsave("./out/figure2b.pdf", width = 6.5, height = 4)


```



Figure 3: Scatterplot of 1st and 2nd degree contacts
```{r}

get_p <- function(params){
  mean((deg2 - params[1] * (1 - exp(-deg * params[2])))^2)
}

best_p <- optim(c(238,0.085), get_p)$par

RMSE <- sqrt(get_p(best_p))

mod_fn <- function(x){
  best_p[1] * (1 - exp(-x * best_p[2]))
}

dat2 <- data.frame(x = deg, y = deg2)

### which estimates are best, most above, most below?
# modeled values
mod_vals <- sapply(deg, mod_fn)

# biggest underestimate
under <- which.min(deg2 - mod_vals)

# closest estimate
closest <- 204

# biggest overestimate
over <- which.max(deg2 - mod_vals)

# pull out these special points and recolor them
scatter_colors <- rep("black", nrow(dat2))
scatter_colors[c(under, closest, over)] <- "green"

ggplot(dat2, aes(x=x,y=y)) +
  geom_function(fun = mod_fn, color = "red") + 
  geom_point(color = scatter_colors) + 
  theme_minimal() +
  labs(
    title = "First- and Second-Degree Contacts, BYU",
    x = "First-Degree Contacts",
    y = "Second-Degree Contacts"
  )

ggsave("./out/figure3b.pdf", width = 6.5, height = 4)

print(mean(deg))
print(sd(deg))
print(range(deg))




```


Figure 4: representative network graphs
```{r}

under_df <- data.frame(x = rep(under, sum(bin_adj[under,])), y = which(bin_adj[under,]))
for(i in which(bin_adj[under,])){
  under_df <- rbind(under_df, data.frame(x = rep(i, sum(bin_adj[i,])), y = which(bin_adj[i,])))
}
under_g <- simplify(graph_from_data_frame(under_df, directed = FALSE))

ggraph(under_g, layout = 'kk') + 
  geom_edge_link(width = 0.2, alpha = 0.5) + 
  geom_node_point(
    aes(
      color = factor(distances(under_g, to = paste(under))),
      size = deg[as.numeric(names(V(under_g)))]
      )
    ) +
  scale_size(name = "Node Degree", range = c(1,10)) +
  scale_color_discrete(name = "Contact Degree") +
  theme(
    panel.background = element_rect(fill = "transparent", colour = NA),
    plot.background = element_rect(fill = "transparent", colour = NA),
    legend.key=element_blank()
  )
ggsave("./out/figure4a.pdf", width = 6.5, height = 4)

####

closest_df <- data.frame(x = rep(closest, sum(bin_adj[closest,])), y = which(bin_adj[closest,]))
for(i in which(bin_adj[closest,])){
  closest_df <- rbind(closest_df, data.frame(x = rep(i, sum(bin_adj[i,])), y = which(bin_adj[i,])))
}
closest_g <- simplify(graph_from_data_frame(closest_df, directed = FALSE))

ggraph(closest_g, layout = 'kk') + 
  geom_edge_link(width = 0.2, alpha = 0.5) + 
  geom_node_point(
    aes(
      color = factor(distances(closest_g, to = paste(closest))),
      size = deg[as.numeric(names(V(closest_g)))]
      )
    ) +
  scale_size(name = "Node Degree", range = c(1,10)) +
  scale_color_discrete(name = "Contact Degree") +
  theme(
    panel.background = element_rect(fill = "transparent",colour = NA),
    plot.background = element_rect(fill = "transparent",colour = NA),
    legend.key=element_blank()
  )
ggsave("./out/figure4b.pdf", width = 6.5, height = 4)

####

over_df <- data.frame(x = rep(over, sum(bin_adj[over,])), y = which(bin_adj[over,]))
for(i in which(bin_adj[over,])){
  over_df <- rbind(over_df, data.frame(x = rep(i, sum(bin_adj[i,])), y = which(bin_adj[i,])))
}
over_g <- simplify(graph_from_data_frame(over_df, directed = FALSE))

ggraph(over_g, layout = 'kk') + 
  geom_edge_link(width = 0.2, alpha = 0.5) + 
  geom_node_point(
    aes(
      color = factor(distances(over_g, to = paste(over))),
      size = deg[as.numeric(names(V(over_g)))]
      )
    ) +
  scale_size(name = "Node Degree", range = c(1,10)) +
  scale_color_discrete(name = "Contact Degree") +
  theme(
    panel.background = element_rect(fill = "transparent",colour = NA),
    plot.background = element_rect(fill = "transparent",colour = NA),
    legend.key=element_blank()
  )
ggsave("./out/figure4c.pdf", width = 6.5, height = 4)

```

Figure 5: regression analyses
```{r}

# Pull in results from Small_Model_BYU.R
load("./final_BYU.RData")

# Time-weighted second degree contact matrix
mat2 <- real_adj %*% real_adj
diag(mat2) <- 0

# Regression results table
table_reg <- data.frame()

# equal weight
mod <- summary(lm(final ~ deg + 0))
table_reg <- rbind(
  table_reg,
  c(
    "Equal-Weighted Contacts",
    paste(round(mod$adj.r.squared, 3)),
    paste(round(mod$coefficients[3], 3)),
    "***" # copied based on results
  )
)

# duration weight
mod <- summary(lm(final ~ colsums(real_adj) + 0))
table_reg <- rbind(
  table_reg,
  c(
    "Duration-Weighted Contacts",
    paste(round(mod$adj.r.squared, 3)),
    paste(round(mod$coefficients[3], 3)),
    "***" # copied based on results
  )
)

# duration weight, 1st + 2nd degree
mod <- summary(lm(final ~ colsums(real_adj) + colsums(mat2) + 0))
table_reg <- rbind(
  table_reg,
  c(
    "Duration-Weighted 1st- and 2nd-Degree Contacts",
    paste(round(mod$adj.r.squared, 3)),
    paste(round(mod$coefficients[,3], 3), collapse = ", "),
    "***, ***" # copied based on results
  )
)

colnames(table_reg) <- c("Predictor(s)", "Adjusted R-Squared", "t Statistic(s)", "Significance Code(s)")

tg = gridExtra::tableGrob(table_reg, rows = NULL)
h = grid::convertHeight(sum(tg$heights), "in", TRUE)
w = grid::convertWidth(sum(tg$widths), "in", TRUE)

pdf("./out/figure5b.pdf", width=w, height=h)
grid.table(table_reg, rows = NULL)
dev.off()


```

Figure 6: Histogram of cumulative cases after 4-weeks (incl. index) - different R0
```{r}

results_R0 <- data.frame()

for (R0 in seq(1,3,0.5)) {
  lambda <- nlm(err, R0/(((sigma2 + mu^2)/mu - 1) * mean(total_time)))$estimate
  new_results <- matrix(unlist(mclapply(1:10000, outbreak, mc.cores = 12)), ncol = 2, byrow = TRUE)
  new_results <- cbind(new_results, rep(R0, nrow(new_results)))
  new_results <- new_results[,2:3]
  results_R0 <- rbind(results_R0, as.data.frame(new_results))
}
colnames(results_R0) <- c("cases", "R0")

####

table_R0 <- data.frame()

for (i in seq(1,3,0.5)) {
  table_R0 <- rbind(
    table_R0,
    c(
      i,
      paste(mean(results_R0$cases[which(results_R0$R0 == i)])),
      paste(median(results_R0$cases[which(results_R0$R0 == i)])),
      paste(quantile(results_R0$cases[which(results_R0$R0 == i)], c(0.025, 0.975), type = 1), collapse = "-")
    )
  )
}

colnames(table_R0) <- c("Reproductive Number", "Mean Cases, 4 Weeks", "Median Cases, 4 Weeks", "Credible Interval")

tg = gridExtra::tableGrob(table_R0, rows = NULL)
h = grid::convertHeight(sum(tg$heights), "in", TRUE)
w = grid::convertWidth(sum(tg$widths), "in", TRUE)

pdf("./out/figure6b.pdf", width=w, height=h)
grid.table(table_R0, rows = NULL)
dev.off()

####

ggplot(results_R0, aes(cases, factor(R0))) + 
  geom_violin(aes(fill = R0), scale = "width") +
  theme_minimal() +
  labs(
    title = "Cumulative Cases after 4 Weeks",
    y = "Basic Reproductive Number",
    x = "Cases"
  )

ggsave("./out/figure6a.pdf", width = 6.5, height = 4)


```

Figure 7: Histogram of cumulative cases after 4-weeks (incl. index) - different testing levels
```{r}

results_testing <- data.frame()

R0 <- 2
lambda <- nlm(err, R0/(((sigma2 + mu^2)/mu - 1) * mean(total_time)))$estimate

for (daily_tests in seq(500,2000,500)) {
  
  new_results <- as.data.frame(matrix(unlist(mclapply(1:10000, outbreak, daily_tests = daily_tests, mc.cores = 12)), ncol = 2, byrow = TRUE))
  new_results <- cbind(new_results, rep(daily_tests, nrow(new_results)))
  new_results <- cbind(new_results, rep("random", nrow(new_results)))
  colnames(new_results) <- c("primary_transmissions", "cases", "daily_tests", "type")
  results_testing <- rbind(results_testing, as.data.frame(new_results))
}

for (daily_tests in seq(500,2000,500)) {
  
  new_results <- as.data.frame(matrix(unlist(mclapply(1:10000, outbreak, daily_tests = daily_tests, test_strategic = TRUE, mc.cores = 12)), ncol = 2, byrow = TRUE))
  new_results <- cbind(new_results, rep(daily_tests, nrow(new_results)))
  new_results <- cbind(new_results, rep("strategic", nrow(new_results)))
  colnames(new_results) <- c("primary_transmissions", "cases", "daily_tests", "type")
  results_testing <- rbind(results_testing, as.data.frame(new_results))
}

table_test_random <- data.frame()

for (i in seq(500,2000,500)) {
  table_test_random <- rbind(
    table_test_random,
    c(
      paste(i),
      paste(mean(results_testing$primary_transmissions[which(results_testing$daily_tests == i & results_testing$type == "random")])),
      paste(mean(results_testing$cases[which(results_testing$daily_tests == i & results_testing$type == "random")])),
      paste(median(results_testing$cases[which(results_testing$daily_tests == i & results_testing$type == "random")])),
      paste(quantile(results_testing$cases[which(results_testing$daily_tests == i & results_testing$type == "random")], c(0.025, 0.975), type = 1), collapse = "-")
    )
  )
}

colnames(table_test_random) <- c("Daily Tests", "Reproductive Number", "Mean Cases, 4 Weeks", "Median Cases, 4 Weeks", "Credible Interval")

tg = gridExtra::tableGrob(table_test_random, rows = NULL)
h = grid::convertHeight(sum(tg$heights), "in", TRUE)
w = grid::convertWidth(sum(tg$widths), "in", TRUE)

pdf("./out/figure7b.pdf", width=w, height=h)
grid.table(table_test_random, rows = NULL)
dev.off()

####

table_test_strategic <- data.frame()

for (i in seq(500,2000,500)) {
  table_test_strategic <- rbind(
    table_test_strategic,
    c(
      paste(i),
      paste(mean(results_testing$primary_transmissions[which(results_testing$daily_tests == i & results_testing$type == "strategic")])),
      paste(mean(results_testing$cases[which(results_testing$daily_tests == i & results_testing$type == "strategic")])),
      paste(median(results_testing$cases[which(results_testing$daily_tests == i & results_testing$type == "strategic")])),
      paste(quantile(results_testing$cases[which(results_testing$daily_tests == i & results_testing$type == "strategic")], c(0.025, 0.975), type = 1), collapse = "-")
    )
  )
}

colnames(table_test_strategic) <- c("Daily Tests", "Reproductive Number", "Mean Cases, 4 Weeks", "Median Cases, 4 Weeks", "Credible Interval")

tg = gridExtra::tableGrob(table_test_strategic, rows = NULL)
h = grid::convertHeight(sum(tg$heights), "in", TRUE)
w = grid::convertWidth(sum(tg$widths), "in", TRUE)

pdf("./out/figure7c.pdf", width=w, height=h)
grid.table(table_test_strategic, rows = NULL)
dev.off()

####

ggplot(results_testing, aes(fill = type, x = cases, y = factor(daily_tests))) + 
  geom_violin(scale = "width", bw = 0.1) +
  theme_minimal() +
  labs(
    title = "Cumulative Cases after 4 Weeks",
    y = "Daily Tests",
    x = "Cases"
  ) +
  scale_x_continuous(trans='log10')

ggsave("./out/figure7a.pdf", width = 6.5, height = 4)


```

Figure 8: Histogram of cumulative cases after 4-weeks (incl. index) - different vaccination rates
```{r}

results_vax <- data.frame()

R0 <- 2
lambda <- nlm(err, R0/(((sigma2 + mu^2)/mu - 1) * mean(total_time)))$estimate

for (p_vax in seq(0.2,0.8,0.2)) {
  
  new_results <- as.data.frame(matrix(unlist(mclapply(1:10000, outbreak, p_vax = p_vax, mc.cores = 12)), ncol = 2, byrow = TRUE))
  new_results <- cbind(new_results, rep(p_vax, nrow(new_results)))
  new_results <- cbind(new_results, rep("random", nrow(new_results)))
  colnames(new_results) <- c("primary_transmissions", "cases", "p_vax", "type")
  results_vax <- rbind(results_vax, as.data.frame(new_results))
}

for (p_vax in seq(0.2,0.8,0.2)) {
  
  new_results <- as.data.frame(matrix(unlist(mclapply(1:10000, outbreak, p_vax = p_vax, vax_strategic = TRUE, mc.cores = 12)), ncol = 2, byrow = TRUE))
  new_results <- cbind(new_results, rep(p_vax, nrow(new_results)))
  new_results <- cbind(new_results, rep("strategic", nrow(new_results)))
  colnames(new_results) <- c("primary_transmissions", "cases", "p_vax", "type")
  results_vax <- rbind(results_vax, as.data.frame(new_results))
}

####

table_vax_random <- data.frame()

for (i in seq(0.2,0.8,0.2)) {
  table_vax_random <- rbind(
    table_vax_random,
    c(
      paste(i * 100, "%", sep = ""),
      paste(mean(results_vax$primary_transmissions[which(results_vax$p_vax == i & results_vax$type == "random")])),
      paste(mean(results_vax$cases[which(results_vax$p_vax == i & results_vax$type == "random")])),
      paste(median(results_vax$cases[which(results_vax$p_vax == i & results_vax$type == "random")])),
      paste(quantile(results_vax$cases[which(results_vax$p_vax == i & results_vax$type == "random")], c(0.025, 0.975), type = 1), collapse = "-")
    )
  )
}

colnames(table_vax_random) <- c("Vaccination Rate", "Reproductive Number", "Mean Cases, 4 Weeks", "Median Cases, 4 Weeks", "Credible Interval")

tg = gridExtra::tableGrob(table_vax_random, rows = NULL)
h = grid::convertHeight(sum(tg$heights), "in", TRUE)
w = grid::convertWidth(sum(tg$widths), "in", TRUE)

pdf("./out/figure8b.pdf", width=w, height=h)
grid.table(table_vax_random, rows = NULL)
dev.off()

####

table_vax_strategic <- data.frame()

for (i in seq(0.2,0.8,0.2)) {
  table_vax_strategic <- rbind(
    table_vax_strategic,
    c(
      paste(i * 100, "%", sep = ""),
      paste(mean(results_vax$primary_transmissions[which(results_vax$p_vax == i & results_vax$type == "strategic")])),
      paste(mean(results_vax$cases[which(results_vax$p_vax == i & results_vax$type == "strategic")])),
      paste(median(results_vax$cases[which(results_vax$p_vax == i & results_vax$type == "strategic")])),
      paste(quantile(results_vax$cases[which(results_vax$p_vax == i & results_vax$type == "strategic")], c(0.025, 0.975), type = 1), collapse = "-")
    )
  )
}

colnames(table_vax_strategic) <- c("Vaccination Rate", "Reproductive Number", "Mean Cases, 4 Weeks", "Median Cases, 4 Weeks", "Credible Interval")

tg = gridExtra::tableGrob(table_vax_strategic, rows = NULL)
h = grid::convertHeight(sum(tg$heights), "in", TRUE)
w = grid::convertWidth(sum(tg$widths), "in", TRUE)

pdf("./out/figure8c.pdf", width=w, height=h)
grid.table(table_vax_strategic, rows = NULL)
dev.off()

####

ggplot(results_vax, aes(fill = type, x = cases, y = factor(p_vax))) + 
  geom_violin(scale = "width", bw = 0.1) +
  theme_minimal() +
  labs(
    title = "Cumulative Cases after 4 Weeks",
    y = "Proportion Vaccinated",
    x = "Cases"
  ) +
  scale_x_continuous(trans='log10')

ggsave("./out/figure8a.pdf", width = 6.5, height = 4)

```




